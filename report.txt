1. preprocess
	a. clean useless information from tweets (i)
		involves a few things. majority of tweets have two numbers at the start, a tweet body and a date and time at the end. we want to just pull the body and get rid of the rest.

		Number of tweets: 3847
		# with date and time at end: 3738
		# with two numbers at start: 3744


discard small words?
get rid of words in tweet that arent locations??????

coudl tokenize but local edit distance as an alg focuses on substrings anyway. toeknizing would make more sense for global edit distance.
		
Abstract
In analysing ~1.3 million locations and ~300k tweets, the number of Levenshtein distance calculations that need to be performed numbers around ~4 billion. This number of calculations is more than large enough to consider algorithmic time complexity as a very important factor. With this in mind, the TRE library was selected, which features a local edit distance algorithm with a reasonable time complexity of `O(m^2 n)`, where `m` is the length of the regex query and `n` is the length of the string being searched. In such a problem, no constant reduction in the amount of work to be done will counterbalance a good algorithm. Nonetheless, several preprocessing steps were taken which cut down the search space quite significantly, ranging from standard procedures like removing punctuation to more complicated steps such as removing all the very common (non-location) words from the tweets. Compared to an algorithm like global edit distance, in which tweets can be tokenized, local edit distance is not as efficient because it must consider the whole tweet and all permutations of it n-edit-distance from the original. This is obviously more complicated than considering permutations of just small, distinct tokens. However the increased complexity comes with its benefits. ???More matches???(isnt this potentially bad also)?????

TRE has a few advantages:
    - extended regex
    - O(m^2 n)
    - Good scaling in time complexity with fuzzyness
    - correctness?
    - parralelizes well.

- treats the tweet as a monolithic entry
- Implemented in C, just a python hook.

Why not tokenize? Doesn't make sense in the context of local edit distance.

Introduction

Preprocessing

Algorithm and Data Structure

Efficiency

Effectiveness (various correctness measures)